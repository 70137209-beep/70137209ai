# -*- coding: utf-8 -*-
"""id3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13_TIKuFhYwz-Tl59l-3DW0oKaOLfqa0Y
"""

import pandas as pd
import numpy as np
import math

data = {
    'Outlook': ['Sunny','Sunny','Overcast','Rain','Rain','Rain','Overcast','Sunny'],
    'Temperature': ['Hot','Hot','Hot','Mild','Cool','Cool','Mild','Cool'],
    'Humidity': ['High','High','High','High','Normal','Normal','Normal','High'],
    'Wind': ['Weak','Strong','Weak','Weak','Weak','Strong','Strong','Weak'],
    'PlayTennis': ['No','No','Yes','Yes','Yes','No','Yes','No']
}

df = pd.DataFrame(data)
df

def entropy(target_col):
    values, counts = np.unique(target_col, return_counts=True)
    entropy_val = 0

    for i in range(len(values)):
        prob = counts[i] / np.sum(counts)
        entropy_val -= prob * math.log2(prob)

    return entropy_val

def information_gain(df, feature, target="PlayTennis"):
    total_entropy = entropy(df[target])
    values, counts = np.unique(df[feature], return_counts=True)

    weighted_entropy = 0
    for i in range(len(values)):
        subset = df[df[feature] == values[i]]
        weighted_entropy += (counts[i] / np.sum(counts)) * entropy(subset[target])

    return total_entropy - weighted_entropy

def id3(df, features, target="PlayTennis"):
    if len(np.unique(df[target])) == 1:
        return np.unique(df[target])[0]

    if len(features) == 0:
        return df[target].mode()[0]

    gains = [information_gain(df, feature, target) for feature in features]
    best_feature = features[np.argmax(gains)]

    tree = {best_feature: {}}
    remaining_features = [f for f in features if f != best_feature]

    for value in np.unique(df[best_feature]):
        subset = df[df[best_feature] == value]
        tree[best_feature][value] = id3(subset, remaining_features, target)

    return tree

features = list(df.columns[:-1])
decision_tree = id3(df, features)

decision_tree